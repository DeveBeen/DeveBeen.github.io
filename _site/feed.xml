<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-04-29T14:30:28+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Minimum Viable Model</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Armando Maynez</name></author><entry><title type="html">Git,Github?</title><link href="http://localhost:4000/Git-Github.html" rel="alternate" type="text/html" title="Git,Github?" /><published>2023-04-29T00:00:00+09:00</published><updated>2023-04-29T00:00:00+09:00</updated><id>http://localhost:4000/Git-Github</id><content type="html" xml:base="http://localhost:4000/Git-Github.html"><![CDATA[<hr />

<h2 id="1-githubgit이란">1. Github,Git이란?</h2>
<ul>
  <li>git이란 Distributed version control system이다. 즉, 직역하면 분산 버전 관리 시스템으로 분산해서 버전을 관리(변화,변경)를 하는 시스템을 말한다.</li>
  <li>github라는 Web Service의 주요 기능인 원격 저장소 역할을 한다.</li>
  <li>GoogleDrive 같은 원격 저장소와는 버전으로 관리 되어 하위 버전 이동 또는 버전 업데이트가 자유롭다.</li>
  <li>github는 이러한 git을 사용하여 현업과 프로젝트 버전관리등의 유용한 기능을 하는 개발자의 커뮤니티이다.</li>
</ul>

<hr />

<h2 id="2-그래서-버전-관리를-왜-하는데">2. 그래서 버전 관리를 왜 하는데?</h2>
<ul>
  <li>다양한 이유가 있지만 먼저 간단하게 시각적으로 보자면 아래와 같은 상황을 방지하기 위함이다.
    <blockquote>
      <p>“조별과제_김원빈.pptx”<br />
“조별과제_김원빈_성준수정.pptx”<br />
“조별과제_김원빈_성준수정_최종.pptx”<br />
“조별과제_김원빈_성준수정_최종_의최종.pptx”<br />
“조별과제_김원빈_성준수정_최종_의최종_을철현수정.pptx”<br />
“조별과제_김원빈_성준수정_최종_의최종_을철현수정_최종.pptx”<br />
 . . .</p>
    </blockquote>
  </li>
  <li>위와 같은 방식을 버전 관리라고 한다. 즉, 과거 파일을 백업해두는 방법이다. 위 같은 방식도 작업하기엔 문제도 없고 백업 기능도 있기에 괜찮아 보일 수 있으나 큰 문제가 있다.</li>
  <li>위 파일은 조별과제이다. 개인이 작업하는게 아니라 조원들이 관리하다가 너도나도 파일을 수정한다면 파일 백업 의미가 없어져 버린다.</li>
  <li>아래와 같은 이유로 깃허브를 사용한다고 할 수 있다.</li>
</ul>

<p>[팀 프로젝트 버전 관리의 문제]<br />
1) 여러 개의 파일 버전을 일관되게 관리하기 힘들다.(누가 가장 최신 파일을 가지고 있는지 모른다.<br />
2) 누가/무엇을/어떻게 변경하였는지 기록,확인,공유가 어렵다.<br />
3) 서로의 변경 내역을 덮어씌우거나 지워버려 오류가 발생할 수 있다.<br />
4) 변경 전 버전을 특정할 수 없으므로 내용을 이전 상태로 돌리기도 어렵다.<br />
<img src="https://user-images.githubusercontent.com/61172021/92212736-65180200-eecd-11ea-8756-eb669f047081.png" alt="image" /></p>

<hr />
<h2 id="3-git-flow-개념-이해하기">3. Git-Flow 개념 이해하기</h2>
<ul>
  <li>git-flow는 과거 Vincent Driessen이라는 사람의 블로그를 통해 알려진 Git 개발 방식의 방법론으로 기능적인 것이 아닌 약속의 의미를 가지고 있는 방법론이다.</li>
  <li>git-flow는 아래와 같이 5가지 <a href="#단어정리">브랜치(branch)</a>로 나뉜다.</li>
</ul>

<p>[Git-Flow 전략]<br /></p>
<ul>
  <li><b>master</b> : 기준인 역할을 하는 브랜치로 제품을 배포하는 최종 브랜치이다.<br /></li>
  <li><b>develop</b> : 개발할 때의 메인 브랜치로. 구현해야할 기능이 발생하면 feature 브랜치를 생성 및 개발, 구현 완료 후 develop 브랜치로 병합한다. 다음 배포 버전까지 개발 완료 후 <a href="#단어정리">QA Test</a>를위해 release 브랜치로 <a href="#단어정리">머지(merge)</a>된다<br /></li>
  <li><b>feature</b> : 이슈 및 기능을 구현하기 위해 생성하는 브랜치로 develop 브랜치에서 생성한다. 기능 구현이 완료되면 develop 브랜치에 머지하고 feature 브랜치는 삭제한다.<br /></li>
  <li><b>release</b> : develop에 저장되어 QA를 진행하여 통과 시 master 브랜치에 머지하는 QA 진행 브랜치이다.<br /></li>
  <li><b>hotfix</b> : 게임을 한 사람들에게는 익숙한 이름의 브랜치로 배포 후 release 브랜치에서 발견 못한 버그가 발생하여 긴급한 수정이 필요할 때 생성하는 브랜치이다. master에서 생성하고 수정 후 master, develop 브랜치에 머지하고 hotfix 브렌치는 삭제한다.<br /></li>
</ul>

<h2 id="4-git-flow-상세">4. Git-Flow 상세</h2>
<ul>
  <li>위에 git-flow에 사용되는 브랜치를 알아봤다. 하지만 저렇게 정의를 해두는 것은 알겠지만 정의만 보고 흐름이 잘 보이지 않는다.</li>
  <li>그렇다면 각 브랜치의 모습을 하나씩 보며 흐름을 이해해보자.</li>
</ul>

<p><b>[master branch]</b><br /></p>
<ul>
  <li>배포가능한 상태를 관리하는 브랜치<br /></li>
</ul>

<p><b>[develop branch]</b><br /></p>
<ul>
  <li>다음 버전을 개발하는 브랜치</li>
  <li>평소 개발 시 기준이 되는 개발 브랜치이다.<br /></li>
</ul>

<p><img src="https://lh4.googleusercontent.com/CZlG9QPr4RYMGAJ3z2ihWV6UuyJRqEmYSwm4Du3AeaFCc2-lrrEG-rWA6YkWKyFvAye_uKv0123vXLt4JY_dey_KkDk8VdPAHvDOgzLg2pwTE0k6li-dL_YUWpP-8Ck8Xrbx4ouS" alt="image" /></p>

<p><b>[feature branch]</b><br /></p>
<ul>
  <li>기능을 개발하는 브랜치로 develop 브랜치에서 뻗어나와 좀 더 세부적으로 개발하기 위한 역할을 하는 보조 브랜치이다.</li>
  <li>feature 브랜치는 기능을 완성할 때까지 유지하며, 완료되면 develop 브랜치에 머지한다.</li>
  <li>물론 기능이 별로다 싶으면 버린다. (develop에 머지하지 않았으니 영향을 미치지 않는다. 그러니 확실하다 싶으면 머지하는 것이 중요하다.)</li>
</ul>

<p><img src="https://lh6.googleusercontent.com/J9X6SYLWwSLiLb6JAd_HBMFeTXpzwIZIMUkqtJpXZzi5cg42gIHLx3F99X-wSVIoFEc0u7NCY08yl-xTFolFlwfR0ytJWxZntoZS3-5WWq_oAlIO_MfJWKQfQYur_8ed7D_vzPF3" alt="image" /></p>

<p><b>[release branch]</b><br /></p>
<ul>
  <li>필터 역할을 하는 브랜치로 develop에서 master로 머지하기 전에 최종 수정을 위해 QA하는 브랜치이다.</li>
  <li>중요한 것은 release 브랜치에서 수정된 사항이 있으면 develop 브랜치에도 머지를 시켜줘야 다음 버전 개발에 꼬이지 않는다.</li>
</ul>

<p><img src="https://lh5.googleusercontent.com/4mXmoEov9sqhCEo6vxFF8eOrvrc5hyo0SvW6YLgJMoauuejV0ketnm9yxjc1JyiUqjZnWwMCQr71JATvU1mAlxk3NPrQcglpWTpaIbIL1aiJbVXJ2e4DSocSo5eeG_I6zOQVfZ9A" alt="image" /></p>

<p><b>[hotfix branch]</b><br /></p>
<ul>
  <li>배포 버전에서 긴급 수정이 필요할 때, master에서 분기하는 브랜치이다.</li>
  <li>긴급 수정을 하는 동시에 develop 브랜치에서 아무러 작용없이 개발할 수 있어 나뉜 브랜치이며, 수정 후 master에 바로 머지를 한다.</li>
  <li>이 또한 중요한 것은 수정 후 release 브랜치처럼 develop 브랜치에도 머지해야하는 것이다.</li>
</ul>

<p><img src="https://lh5.googleusercontent.com/_jcNDU-WEGylP-1Z5CFOIYBDjwOmaqUi6DslzKGZ39rts9IXEEBdyq7NvF1jrlXnLg2dn_mL-tnvINUrUFSx4UOlAkOov_EpwW6eF1zRHYEK8xRB__GyG5HrpEWWFjHNa23WhF8I" alt="image" /></p>

<ul>
  <li>최종적으로 보는 흐름</li>
</ul>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdSKdav%2FbtrCVRy7XUT%2FLKQdnMfcWPsCPf6ogKMq90%2Fimg.png" alt="image" /></p>

<h2 id="단어정리">단어정리</h2>
<ul>
  <li>브랜치(branch) : 직역하면 나뭇가지란 뜻으로 직역 뜻과 같이 나뭇가지 처럼 뻗어나가는 가지 중 한가지를 말하며 Git 개발 작업에서는 이를 하나의 작업라인을 말할 때 사용한다.</li>
  <li>QA Test : Quality Assurance Test로 품질 보증 시험으로 소프트웨어가 개발 되었을 때, 이것이 적절한 디자인 패턴을 적재적소에 사용해 확작성이 있고 유지보수가 좋은 코드인지, 기존 소프트웨어 보다 좋은 또는 편리한 기능을 제시하는지, 기존 소프트웨어보다 성능면에서 좋은지 검증하는 단계라 볼 수 있다.</li>
  <li>머지(merge) : 병합이라는 뜻을 가진 것처럼 나뭇가지로 뻗어나간 수 많은 나뭇가지 중 어떠한 나뭇가지인 작업라인을 또다른 작업라인으로 병합 시켜준다는 말이다.</li>
</ul>]]></content><author><name>Armando Maynez</name></author><category term="opinion" /><category term="front-end" /><category term="git" /><category term="basic" /><summary type="html"><![CDATA[Git,Github가 무엇인지 알아보고, Git Flow 개념을 이해해보자.]]></summary></entry><entry><title type="html">JavaBasic</title><link href="http://localhost:4000/Java-Basic.html" rel="alternate" type="text/html" title="JavaBasic" /><published>2023-04-29T00:00:00+09:00</published><updated>2023-04-29T00:00:00+09:00</updated><id>http://localhost:4000/Java-Basic</id><content type="html" xml:base="http://localhost:4000/Java-Basic.html"><![CDATA[]]></content><author><name>Armando Maynez</name></author><category term="opinion" /><category term="front-end" /><category term="java" /><category term="basic" /><summary type="html"><![CDATA[Java가 무엇인지 알아보고 기초 문법에 대해 알아보자.]]></summary></entry><entry><title type="html">JSON,XML,YAML</title><link href="http://localhost:4000/JSON-XML-YAML.html" rel="alternate" type="text/html" title="JSON,XML,YAML" /><published>2023-04-28T00:00:00+09:00</published><updated>2023-04-28T00:00:00+09:00</updated><id>http://localhost:4000/JSON-XML-YAML</id><content type="html" xml:base="http://localhost:4000/JSON-XML-YAML.html"><![CDATA[<hr />
<h2 id="1-json-javascript-object-notation">1. JSON (JavaScript Object Notation)</h2>
<ul>
  <li>데이터를 저장, 전송할 때 사용되는 경량의 DATA 교환 형식 JavaScript에서 객체를 만들 때 사용하는 단순한 텍스트 형식인 표현식이다.</li>
  <li>클라이언트와 서버가 주고 받는 요청(Request)와 응답(Response) 사이에 담겨있는 데이터의 형식 중 하나이다.
-JSON은 데이터 포멧일 뿐이고 단순히 데이터를 표시하는 방법이다.</li>
</ul>

<p>[JSON 형식의 장점]</p>
<ul>
  <li>대부분의 프로그래밍 언어에서 JSON을 핸들링할 수 있도록 라이브러리를 제공한다.</li>
  <li>XML보다 최소한의 용량으로 데이터 전송이 가능하며, 구조 정의의 용이성과 가독성이 좋다.</li>
  <li>내용이 최소한의 정보만 가지고 있기에 용량이 줄어들고 빠른 속도를 가지며, 언어가 독립적이고 사용하기 쉽다.</li>
</ul>

<p>[JSON 형식의 단점]</p>
<ul>
  <li>장점과 반대로 내용이 최소한의 정보만 가지고 있어 의미파악이 힘들다.</li>
  <li>경량의 데이터 교환형식이기에 대용량의 데이터 송수신에는 부적합하다.</li>
  <li>XML은 사용처마다 요구되는 구조와 형태를 잘 갖췄는지 스키마를 이용해 검증이 가능하지만 JSON은 해당 기능을 지원하지 않는다.</li>
  <li>주석을 달지 못하고, 문법 오류에 취약하다.</li>
</ul>

<p><img src="https://media.licdn.com/dms/image/C560DAQEXmiSIYftM5Q/learning-public-crop_288_512/0/1629482233685?e=2147483647&amp;v=beta&amp;t=Cxx1JriYNY21m914uW3MS3YEhig1ezopPqS0941wD7s" alt="image" /></p>

<p>[JSON 형식의 구조]</p>
<ul>
  <li>JSON 형식의 구조는 key:value 형태의 구조를 가지고 있다. 중괄호 {}로 감싸며, 이는 객체(Object)가 나올 것을 의미한다.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  "나이":30,
  "이름":"홍길동",
  "특기":["달리기","수영"]
}
</code></pre></div>    </div>
    <hr />
    <h2 id="2-xml-extensible-markup-language">2. XML (Extensible Markup Language)</h2>
  </li>
  <li>XML은 데이터를 정의하는 규칙을 제공하는 markup 언어로 다른 프로그래밍 언어와는 다르게 자체적으로 컴퓨팅하는 작업을 수행할 수 없다.</li>
  <li>대신 구조적 데이터 관리를 위해 모든 프로그래밍 언어 또는 소프트웨어를 구현할 수 있다.</li>
  <li>태그라는 markup 기호를 사용하여 문서 내용에 영향을 주지 않고 사용하여 데이터의 유용성을 높인다.</li>
</ul>

<p>[XML의 특징]</p>
<ul>
  <li>표준성 : <a href="#단어정리">W3C</a>에서 표준화를 주도, SGML과 HTML의 한계를 극복하기 위해 만든 표준 인터넷 언어이다.</li>
  <li>분리성 : 표현과 내용이 분리되어 있고 XML 문서는 데이터의 구조와 내용을 기술하고 있으며, 스타일 시트를 이용하여 다양한 방식으로 데이터를 표현한다.</li>
  <li>단순성 : XML 문서는 텍스트로 간단하게 되어 있어 하드웨어나 소프트웨어에 의존하지 않고 읽어 들일 수 있다는 장점이 있다.</li>
  <li>호환성 : 위 단순성의 특성으로 다양한 시스템간에 상호 작용을 중계하는데 XML 사용이 가능하다.</li>
  <li>수용성 : HTML과 같이 현재 인터넷에서 가장 많이 사용되는 HTTP 프로토콜을 이용하여 전달한다.</li>
  <li>확장성 : XML은 확장성 있는 태그를 사용, 어떤 분야의 데이터도 명확하게 기술이 가능하다.</li>
  <li>SEQ : 문서의 의미 있는 태그를 사용하여 원하는 데이터를 쉽게 찾을 수 있는 장점이 있어 검색엔진최적화(SEQ)에 유리하다.
<img src="https://i0.wp.com/hanamon.kr/wp-content/uploads/2021/06/HTML-vs-XML.png?fit=1280%2C844&amp;ssl=1" alt="image" />
    <blockquote>
      <h3 id="xml과-html">[XML과 HTML]<br /></h3>
      <ul>
        <li>XML은 Data를 전달하는 것에 포커스를 맞춘 언어라면 HTML은 Data를 표현하는 것에 포커스를 맞춘 언어이다. <br /></li>
        <li>HTML의 태그는 이미 약속한 태그만 사용이 가능하지만 XML은 사용자가 임의로 만들어서 사용이 가능합니다.
~~~
&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;</li>
      </ul>
    </blockquote>
  </li>
</ul>
<마트>
    <과일류>
        사과, 바나나
    </과일류>
    <채소류>
        양배추
    </채소류>
    <라면류>
        일품라면
    </라면류>
</마트>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
## 3. YAML (YAML Ain't Makeup Language)
- YAML도 데이터 표현 양식의 한 종류이며 아래와 같은 특징을 가진다.

[YAML의 특징]
- yaml은 인간이 보고 이해하기 쉬운 형태를 가지고 있어 최근 많이 활용되는 데이터 포멧입니다.
- 기본적으로 들여쓰기를 원칙으로 하며 JSON과 비슷하게 Key:Value 형태를 가지고 있다.

</code></pre></div></div>
<p>Servers:
    - name : Server1
      administrator : Kim
      created : 20050103132749
      status : active
    - name : Server2
      administrator : Lee
      created : 20210101000000
      status : active
~~~
—</p>
<h2 id="단어정리">단어정리</h2>
<ul>
  <li>W3C(World Wide Web Consortium) : 월드 와이드 웹을 위한 표준을 개발하고 관리하는 조직으로 팀 버너스리를 중심으로 설립된 조직이다.</li>
</ul>]]></content><author><name>Armando Maynez</name></author><category term="opinion" /><category term="front-end" /><category term="datatype" /><category term="basic" /><summary type="html"><![CDATA[JSON,XML,YAML이 각각 무엇인지 그 차이가 뭔지 알아보자.]]></summary></entry><entry><title type="html">CSR/SSR,MAP/SPA</title><link href="http://localhost:4000/CSR-SSR-MAP-SPA.html" rel="alternate" type="text/html" title="CSR/SSR,MAP/SPA" /><published>2023-04-17T00:00:00+09:00</published><updated>2023-04-17T00:00:00+09:00</updated><id>http://localhost:4000/CSR-SSR-MAP-SPA</id><content type="html" xml:base="http://localhost:4000/CSR-SSR-MAP-SPA.html"><![CDATA[<hr />
<h2 id="1-csr-client-side-renderring">1. CSR (Client Side Renderring)</h2>
<ul>
  <li>CSR(Client Side Rendering)은 말 그대로 Client 측에서 <a href="#단어정리">렌더링(Rendering)</a>이 이루어지는 방식을 말한다. CSR의 작업은 다음과 같은 순서로 이루어진다.<br />
    <blockquote>
      <p>1) 클라이언트(브라우저)가 웹 서버에게 요청을 보낸다.<br />
2) Web 서버는 비어있는 HTML 문서인 index.html을 반환한다.(index.html은 비어있는 파일)<br />
3) HTML 문서에 있는 어플리케이션 자바스크립트 링크를 참고하여 웹 서버로부터 자바스크립트 파일을 다운받는다.<br />
4) 추가로 필요한 데이터가 있으면 서버에서 추가적으로 요청하여 가져온다.<br />
5) 사용자에게 화면을 보여준다.<br /></p>
    </blockquote>
  </li>
</ul>

<p>[CSR 방식의 장점]</p>
<ul>
  <li>빠른 <a href="#단어정리">인터렉션</a>을 구현할 수 있다.</li>
  <li>View Rendering을 브라우저에게 담당시키므로서 서버 트레픽을 감소시키고, 사용자에게 더 빠른 인터렉션을 제공해준다.</li>
  <li>새로고침이 발생하지 않아 사용자가 불편함을 느낄 수 있습니다.</li>
</ul>

<p>[CSR 방식의 단점]</p>
<ul>
  <li>첫 페이지 로딩 속도가 Server Side Rendering 보다 다소 느립니다.</li>
  <li><a href="#단어정리">검색엔진최적화(SEO)</a>에 대한 추가 보완 작업이 필요하다.</li>
</ul>

<h2 id="2-ssr-server-side-rendering">2. SSR (Server Side Rendering)</h2>
<ul>
  <li>SSR(Server Side Rendering)은 CSR 방식과 다르게 Server에서 User에게 보여줄 화면을 미리 페이지를 모두 구성하여 렌더링이 이루어지는 방식을 말한다. SSR의 작업은 다음과 같은 순서로 이루어진다.
    <blockquote>
      <p>1) CSR과 다르게 웹 서버에서 비어있는 HTML 파일이 아닌 완성된 HTML 문서와 동적으로 처리 가능한 코드를 가져온다.<br />
2) CSR과 다르게 서버에 두번 요청할 필요없이 클라이언트에서 바로 User에게 화면을 보여준다.</p>
    </blockquote>
  </li>
</ul>

<p>[SSR 방식의 장점]</p>
<ul>
  <li>완성된 형태의 HTML 파일을 서버로 부터 받으므로 크롤링하기에 적합, 검색엔진최적화(SEO) 관점에서 유리하다.</li>
  <li>서버에 이미 로딩을 완료해서오므로 첫 로딩 시간이 매우 짧다.</li>
</ul>

<p>[SSR 방식의 단점]</p>
<ul>
  <li>페이지 요청시 서버에서 페이지를 받아오므로 깜박이는 현상이 나타나 UX는 좋지 않다는 단점이 있다.</li>
  <li>화면에서 바꾸지 않아도 되는 부분을 계속하여 렌더링을 하여 서버가 과부화되기 쉽다.</li>
  <li>사용자가 동적 움직임을 보일 때, 서버에서 계속하여 데이터를 가져오기 때문에 과부하되기 쉽다.</li>
  <li>처음 HTML 문서를 받을 때, <a href="#단어정리">TTV</a>가 먼저 일어나고 이후 클릭을 해야 <a href="#단어정리">TTI</a>가 일어나기에 사용자가 빠르게 웹 사이트를 볼 수 있지만 TTV와 TTI의 공백이 길어 반응이 없을 수도 있다.</li>
</ul>

<h2 id="3-spa-single-page-aplication">3. SPA (Single Page Aplication)</h2>
<ul>
  <li>SPA는 CSR 방식을 사용하는 하나의 Page로 구성된 Application이다. 하지만 react, vue, angular 등이 있고 특히 react는 Next.js와 Gatsby.js에서 SSG을 제공하기 때문에 SPA라고 모두 CSR 방식을 사용하는 것은 아니다.</li>
  <li>처음 요청 시 딱 한 페이지만 불러오고 페이지 이동 시 기존 페이지에서 내부를 수정하여 보여주는 방식이다. 이에 페이지 리로딩 없이 서버에서 내부 수정 시 필요한 데이터만 가져온다.</li>
</ul>

<p>[SPA 방식의 장점]</p>
<ul>
  <li>필요한 리소스만 부분적으로 로딩하여 클라이언트가 처음 요청한 정적 리소스와 추가적인 데이터는 모두 캐시에 저장된다.</li>
  <li>페이지의 부분적인 요소만 갱신하기 때문에 전체적으로 트레픽이 감소한다.</li>
  <li>페이지를 새로고침하지 않기 때문에 <a href="#단어정리">UX(User Experience)</a>에서 매우 좋다.</li>
</ul>

<p>[SPA 방식의 단점]</p>
<ul>
  <li>Web Application에 필요한 정적 리소스를 한번에 다운로드 받기에 초기에 구동 속도가 느리다.</li>
  <li>검색 엔진이 크롤링할 때, Application이 로드 되기 전에 빈 상태의 코드를 크롤링하게 검색엔진최적화(SEO)가 어렵다.</li>
  <li>SSR에선 사용자의 정보를 서버측에서 관리하지만 CSR 방식은 클라이언트에서 가져오기에 정보를 저장할 공간이 마땅치 않아 보안이 취약하다.</li>
</ul>

<h2 id="4-mpa-multiple-page-application">4. MPA (Multiple Page Application)</h2>
<ul>
  <li>MPA는 SSR 방식을 사용하는 여러개의 Page로 구성된 Application이다. SPA와는 다르게 새로운 페이지를 요청할 때마다 정적 리소스를 가져와 전체 페이지를 다시 렌더링한다.</li>
</ul>

<p>[MPA 방식의 장점]</p>
<ul>
  <li>User가 보는 화면이 Web 크롤링 화면이랑 방식이 같아 SEO에 좋다.</li>
  <li>SPA 방식보다 써오던 방식과 역사가 길어 초보자들도 다양한 튜토리얼,해결법,프레임워크로 좋은 환경에서 제작할 수 있다.</li>
</ul>

<p>[MPA 방식의 단점]</p>
<ul>
  <li>다른 페이지 이동 시, 페이지를 새로 로딩하기에 UX가 좋지 않다.</li>
  <li>요청이 들어올 때마다 새로 패치하기에 속도가 느리다.</li>
  <li>프론트앤드와 백앤드가 확실하게 구분되어 있지 않고 연결성이 높아 서버쪽,클라이언트쪽 언어가 모두 필요하다.
<img src="https://velog.velcdn.com/images/wns450/post/13be25ec-96a5-47c7-bc85-6ad0acb5d01e/image.png" alt="image" title="정리사진" /></li>
</ul>

<h2 id="단어정리">단어정리</h2>
<ul>
  <li>렌더링(Rendering) : html.js 파일을 각 단계를 거쳐 User에게 보여주는 것으로 각 단계는 아래와 같다.
    <blockquote>
      <p>1) Parsing : 브라우저가 html 파일을 읽어 트리 자료형으로 해석하는 단계<br /> 
2) Style : parsing 단계에서 만들어진 트리에서 실제로 브라우저에 띄울 데이터를 트리로 자료형으로 정리해주는 단계<br />
3) Layout : Style 작업으로 만들어진 트리의 요소를 화면에 어떻게 배치할지 계산해주는 단계<br />
4) Paint : Layout 단계에서 계산된 값을 화면상에 실제 픽셀로 찍어주는 단계<br />
5) Composite : Paint 단계에서 생성된 레이어를 최종 합성해서 화면에 띄워주는 단계
<br /></p>
    </blockquote>
  </li>
  <li>인터렉션(Interation) : 단어의 뜻 그대로 Interface와 User가 서로 행동과 반응이 이어지 듯, 두 개체의 상호작용을 이야기 한다.</li>
  <li>검색엔진최적화(SEO:Search Engine Optimization) : 검색엔진 결과 페이지에서 사용자의 검색 결과에 대하여 웹 페이지의 상위 노출도를 높이는 작업이다.</li>
  <li>TTV(Time To View) : 사용자가 웹 사이트를 볼 수 있는 시간이다.</li>
  <li>TTI(Time To Interact) : 사용자가 웹에서 동적인 활동이 가능하게 되는 시간이다.</li>
  <li>UX(User Experience) : 단어의 뜻 그래도 사용자의 경험이란 뜻으로 UI에서 느끼는 사용자들의 느낌, 태도, 행동 등을 말하며 UX가 좋을 수록 사용자는 UI에서 편안함을 느낀다 할 수 있따.</li>
</ul>]]></content><author><name>Armando Maynez</name></author><category term="opinion" /><category term="front-end" /><category term="basic" /><summary type="html"><![CDATA[CSR/SSR,MAP/SPA이 각각 무엇인지 그 차이가 뭔지 알아보자.]]></summary></entry><entry><title type="html">Interface란?</title><link href="http://localhost:4000/Interface.html" rel="alternate" type="text/html" title="Interface란?" /><published>2023-04-16T00:00:00+09:00</published><updated>2023-04-16T00:00:00+09:00</updated><id>http://localhost:4000/Interface</id><content type="html" xml:base="http://localhost:4000/Interface.html"><![CDATA[<hr />
<h2 id="1-interface란">1. Interface란?</h2>
<ul>
  <li>인터페이스(interface)란 서로 다른 두 개 이상의 독립된 시스템 또는 장치 간의 정보를 교환하는 <a href="#단어정리">공유경계(shared boundary)</a>를 말한다.</li>
  <li>컴퓨터와 사용자의 간에 통신이 가능하도록 하는 프로그램 또는 장치를 말한다.</li>
  <li>이러한 interface는 상호작용하는 대상 관계에 따라 크게 하드웨어 인터페이스, 소프트웨어 인터페이스, 사용자 인터페이스로 3가지로 나눌 수 있습니다.</li>
</ul>

<hr />
<h2 id="2-하드웨어-인터페이스-hardware-interface">2. 하드웨어 인터페이스 (Hardware Interface)</h2>
<ul>
  <li>상호작용하는 대상이 물리적인 기기인 인터페이스로 서로 다른 물리적인 기기를 연결해주는 역할을 한다.</li>
  <li>대표적인 하드웨어 인터페이스로는 <b>USB Interface</b>가 있다.</li>
  <li>USB는 ‘Universal Serial Bus’의 약자로 컴퓨터와 주변 장치를 연결하기 위해 1996년에 만들어진 통일된 연결방법으로 대표적인 하드웨어 인터페이스이다.</li>
</ul>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FTzZyj%2FbtqUM9hdCm0%2FuiBwRgXts1ewIdv5s5fUG1%2Fimg.png" alt="Hartware Interface" title="하드웨어 인터페이스" /></p>

<hr />
<h2 id="3-소프트웨어-인터페이스-software-interface">3. 소프트웨어 인터페이스 (Software Interface)</h2>
<ul>
  <li>상호작용하는 대상이 소프트웨어와 하드웨어로 이를 소프트웨어를 통하여 하드웨어의 동작을 지시하고 제어할 수 있도록 연결해주는 역할을 한다.</li>
  <li>대표적인 소프트웨어 인터페이스로는 <b>API</b>가 있다.</li>
  <li>API는 ‘Application Programming Interface’의 약자로 응용 프로그램 간에 호환이 가능하도록 상호 작용하는 방식을 정해둔 대표적인 소프트웨어 인터페이스이다.</li>
</ul>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMG3RQ%2FbtqUMatVnP5%2FIcOnawRkZ9lr9nQ2Jp7uB0%2Fimg.png" alt="Software Interface" title="소프트웨어 인터페이스" /></p>

<hr />
<h2 id="4-사용자-인터페이스-user-interface">4. 사용자 인터페이스 (User Interface)</h2>
<ul>
  <li>상호작용하는 대상이 사람과 컴퓨터로 사용자가 컴퓨터를 제어할 수 있도록 연결해주는 역할을 한다.</li>
  <li>초기에는 CLI(Command Line Interface)라는 인터페이스를 통하여 사용자가 컴퓨터가 수행할 작업을 지시하고 제어했으나 사용하기 까다로워 사용자가 사용하기에 어렵지 않은 대표적인 사용자 인터페이스인 <b>GUI</b>를 만들었다.</li>
  <li>GUI는 ‘Graphical User Interface’의 약자로 그래픽 요소를 사용해 컴퓨터에게 보내는 명령을 직관적으로 알기 쉽게 아이콘 등으로 나타낸 대표적인 사용자 인터페이스이다.</li>
</ul>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbQkIFN%2FbtqUzuU3ogu%2FnHCg9G9BDJ5ueKfCUn9eP1%2Fimg.png" alt="User Interface" title="사용자 인터페이스" /></p>

<hr />
<h2 id="5-interface-마무리">5. Interface 마무리</h2>
<ul>
  <li>본 게시글은 위와 같이 Interface란 무엇인지에 관하여 사전적 의미를 정리했다. 하지만 앞으로 front-end 개발을 위해 정의 뿐만 아니라 Interface를 어떻게 구성해야 하는지 알고 넘어갈 필요가 있다.</li>
  <li>Interface의 중요한 점은 직관성(Intution)이다. 다음 사진을 보고 어디에 사용하는 물건인지 생각해보자.
<img src="https://www.dhresource.com/0x0/f2/albu/g4/M01/5B/B8/rBVaEFnA4CCAPRiYAAC8hVrHylQ515.jpg" alt="PopcornMachine" title="PopcornMachine" /></li>
  <li>사진을 보았을 땐 폭탄이 터질 것 같은 모습에 압력 장치, 잠금 장치도 보이는 흡사 무시무시한 무기를 저장할 것 같은 그런 기계로 보인다. 하지만 놀랍게도 이는 그냥 팝콘 튀기는 장치이다. 대부분의 사람들은 팝콘을 알지만 저 물건을 맞춘 사람은 소수일 것이다.</li>
  <li>이는 저 기계가 사용자가 사용하기엔 직관성이 떨어지는 인터페이스들을 가지고 있기 때문이다. 이번엔 다른 예시를 보자.
<img src="https://ditoday.com/wp-content/uploads/2019/04/1904-digital-insight-know-how-user.x-ui-03.jpg" alt="다리미" title="다리미" /></li>
  <li>이 사진을 보자마자 여러분은 이것이 어디에 사용하는 물건인지 바로 알아차렸을 것이라 생각한다. 두꺼운 철판과 손잡이 그리고 버튼들 이 인터페이스의 구성으로 우리는 하드웨어의 정체성을 바로 파악할 수 있었다.</li>
  <li>이처럼 인터페이스는 상호작용의 경계면의 역할 뿐이 아니라 존재 자체로 우리에게 정보를 전달 해주는 역할도 한다고 볼 수 있다.</li>
  <li>위와 같은 예시로 Interface를 통하여 생각보다 많은 정보를 사용자에게 줄 수 있다는 점에서 다양한 시각으로 구성해야 한다는 점을 알 수 있다.</li>
</ul>

<hr />
<h2 id="단어정리">단어정리</h2>
<ul>
  <li>공유경계(shared boundary) : 특정기준으로 나누어진 사물 또는 공간을 공유하는 교집합의 빈 공간을 말한다. 즉, A와 B의 교집합이 공집합인 A와 B의 공유경계 S는 각각 A,B의 교집합 관계에서 공집합을 가지지 않음을 말한다.</li>
</ul>

<p><img src="./assets/img/posts/20230416/shared_boundary.jpeg" alt="shared boundary" /></p>]]></content><author><name>Armando Maynez</name></author><category term="opinion" /><category term="front-end" /><category term="basic" /><summary type="html"><![CDATA[Interface가 무엇인지, 종류와 Java언어로 기본적인 인터페이스 구성을 알아보자]]></summary></entry><entry><title type="html">Who owns the copyright for an AI generated creative work?</title><link href="http://localhost:4000/AI-and-intellectual-property.html" rel="alternate" type="text/html" title="Who owns the copyright for an AI generated creative work?" /><published>2021-04-20T00:00:00+09:00</published><updated>2021-04-20T00:00:00+09:00</updated><id>http://localhost:4000/AI-and-intellectual-property</id><content type="html" xml:base="http://localhost:4000/AI-and-intellectual-property.html"><![CDATA[<p>실험</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/tjzOzuKQhSM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>The project was created by <a href="https://overthebridge.org">Over the Bridge</a>, an organization dedicated to increase awareness on mental health and substance abuse in the music industry, trying to denormalize and remove the glamour around such illnesses within the music community.</p>

<p>They are using Google’s <a href="https://magenta.tensorflow.org">Magenta</a>, which is a neural network that precisely was conceived to explore the role of machine learning within the creative process. Magenta has been used to create a brand new “Beatles” song or even there was a band that <a href="https://arstechnica.com/gaming/2019/08/yachts-chain-tripping-is-a-new-landmark-for-ai-music-an-album-that-doesnt-suck/">used it to write a full album</a> in 2019.</p>

<p>So, while reading the article, my immediate thought was: who owns the copyright of these new songs?</p>

<p>Think about it, imagine one of this new songs becomes a massive hit with millions of youtube views and spotify streams, who can claim the royalties generated?</p>

<p>At first it seems quite simple, <em>Over the Bridge</em> should be the ones reaping the benefits, since they are the ones who had the idea, gathered the data and then fed the neural network to get the “work of art”. But in a second thought, didn’t the original artists provide the basis for the work the neural network generated? shouldn’t their state get credit? what about Google whose tool was used, should they get credit too?</p>

<p>Neural networks have been also used to create poetry, paintings and to write news articles, but how do they do it? A computer program developed for machine learning purposes is an algorithm that “learns” from data to make future decisions. When applied to art, music and literary works, machine learning algorithms are actually learning from some input data to generate a new piece of work, making independent decisions throughout the process to determine what the new work looks like. An important feature of this is that while programmers can set the parameters, the work is actually generated by the neural network itself, in a process akin to the thought processes of humans.</p>

<p>Now, creative works qualify for copyright protection if they are original, with most definitions of originality requiring a human author. Most jurisdictions, including <a href="https://www.wipo.int/wipolex/en/details.jsp?id=1319">Spain</a> and <a href="https://dejure.org/gesetze/UrhG/7.html">Germany</a>, specifically state that only works created by a human can be protected by <a href="https://www.wipo.int/copyright/en/">copyright</a>. In the United States, for example, <a href="https://copyright.gov/comp3/chap300/ch300-copyrightable-authorship.pdf">the Copyright Office has declared</a> that it will “register an original work of authorship, provided that the work was created by a human being.”</p>

<p>So as we currently stand, a human author is required to grant a copyright, which makes sense, there is no point of having a neural network be the beneficiary of royalties of a creative work (no bank would open an account for them anyways, lol).</p>

<p>I think amendments have to be made to the law to ensure that the person who undertook all the arrangements necessary for the work to be created by the neural network gets the credit but also we need to modify copyright law to ensure the original authors of the body of work used as data input to produce the new piece get their corresponding share of credit. This will get messy if someone uses for example the #1 song of every month in a decade to create the decade song, then there would be as many as 120 different artists to credit.</p>

<tweet>In a computer generated artistic work, both the person who undertook all the arrangements necessary for its creation as well as the original authors of the data input need to be credited.</tweet>

<p>There will still be some ambiguity as to who undertook the arrangements necessary, only the one who gathered the data and pressed the button to let the network learn, or does the person who created the neural network’s model also get credit? Shall we go all the way and say that even the programmer of the neural network gets some credit as well?</p>

<p>There are some countries, in particular the UK where some progress has been made to amend copyright laws to cater for computer generated works of art, but I believe this is one of those fields where technology will surpass our law making capacity and we will live under a grey area for a while, and maybe this is just what we need, by having these works ending up free for use by anyone in the world, perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living, and thus they can become free to explore their art.</p>

<tweet>Perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living.</tweet>

<p><img src="./assets/img/posts/20210420/post8-rembrandt2.jpg" alt="The next Rembrandt" />
<small><a href="https://www.jwt.com/en/work/thenextrembrandt">The Next Rembrandt</a> is a computer-generated 3-D–printed painting developed by a facial-recognition algorithm that scanned data from 346 known paintings by the Dutch painter in a process lasting 18 months. The portrait is based on 168,263 fragments from Rembrandt’s works.</small></p>]]></content><author><name>Armando Maynez</name></author><category term="opinion" /><category term="copyright" /><category term="creativity" /><category term="neural networks" /><category term="machine learning" /><category term="artificial intelligence" /><summary type="html"><![CDATA[As neural networks are used more and more in the creative process, text, images and even music are now created by AI, but who owns the copyright for those works?]]></summary></entry><entry><title type="html">So, what is a neural network?</title><link href="http://localhost:4000/back-to-basics.html" rel="alternate" type="text/html" title="So, what is a neural network?" /><published>2021-04-02T00:00:00+09:00</published><updated>2021-04-02T00:00:00+09:00</updated><id>http://localhost:4000/back-to-basics</id><content type="html" xml:base="http://localhost:4000/back-to-basics.html"><![CDATA[<p>The omnipresence of technology nowadays has made it commonplace to read news about AI, just a quick glance at today’s headlines, and I get:</p>
<ul>
  <li><a href="https://www.morningbrew.com/emerging-tech/stories/2021/03/29/one-biggest-advancements-ai-also-sparked-fierce-debate-heres?utm_source=morning_brew">This Powerful AI Technique Led to Clashes at Google and Fierce Debate in Tech.</a></li>
  <li><a href="https://fortune.com/2021/04/02/ai-forecasting-supply-chain-factories-caterpillar-agco/">How A.I.-powered companies dodged the worst damage from COVID</a></li>
  <li><a href="https://www.mobihealthnews.com/news/emea/ai-technology-detects-ticking-time-bomb-arteries">AI technology detects ‘ticking time bomb’ arteries</a></li>
  <li><a href="https://www.genengnews.com/insights/ai-in-drug-discovery-starts-to-live-up-to-the-hype/">AI in Drug Discovery Starts to Live Up to the Hype</a></li>
  <li><a href="https://www.c4isrnet.com/artificial-intelligence/2021/04/02/pentagon-seeks-commercial-solutions-to-get-its-data-ready-for-ai/">Pentagon seeks commercial solutions to get its data ready for AI</a></li>
</ul>

<p>Topics from business, manufacturing, supply chain, medicine and biotech and even defense are covered in those news headlines, definitively the advancements on the fields of artificial intelligence, in particular machine learning and deep neural networks have permeated into our daily lives and are here to stay. But, do the general population know what are we talking about when we say “an AI”?  I assume most people correctly imagine a computer algorithm or perhaps the more adventurous minds think of a physical machine, an advanced computer entity or even a robot, getting smarter by itself with every use-case we throw at it. And most people will be right, when “an AI” is mentioned it is indeed an algorithm run by a computer, and there is where the boundary of their knowledge lies.</p>

<p>They say that the best way to learn something is to try to explain it, so in a personal exercise I will try to do an ELI5 (<strong>E</strong>xplain it <strong>L</strong>ike <strong>I</strong> am <strong>5</strong>) version of what is a neural network.</p>

<p>Let’s start with a little history, humans have been tinkering with the idea of an intelligent machine for a while now, some even say that the idea of artificial intelligence was conceived by the ancient greeks (<a href="https://www.thinkautomation.com/bots-and-ai/a-history-of-automation-the-rise-of-robots-and-ai/">source</a>), and several attempts at devising “intelligent” machines have been made through history, a notable one was ‘The Analytical Engine’ created by Charles Babbage in 1837:</p>

<p><img src="./assets/img/posts/20210402/post7-analytical-engine.jpg" alt="The Analytical Engine" />
<small>The Analytical Engine of Charles Babbage - 1837</small></p>

<p>Then, in the middle of last century by trying to create a model of how our brain works, Neural Networks were born. Around that time, Frank Rosenblatt at Cornell trying to understand the simple decision system present in the eye of a common housefly,  proposed the idea of a <a href="./single-neuron-perceptron.html">perceptron</a>, a very simple system that processes certain inputs with basic math operations and produces an output.</p>

<p><img src="./assets/img/posts/20210125/Perceptron.png" alt="A perceptron" /></p>

<p>To illustrate, let’s say that the brain of the housefly is a perceptron, its inputs are whatever values are produced by the multiple cells in its eyes, when the eye cell detects “something” it’s output will be a 1, and if there is nothing a 0. Then the combination of all those inputs can be processed by the perceptron (the fly brain), and the output is a simple 0 or 1 value. If it is a 1 then the brain is telling the fly to flee and if it is a 0 it means it is safe to stay where it is.</p>

<p><img src="./assets/img/posts/20210402/post7-housefly-eye.jpg" alt="A housefly eye" /></p>

<p>We can imagine then that if many of the eye cells of the fly produce 1s, it means that an object is quite near, and therefore the perceptron will calculate a 1, it is time to flee.</p>

<p><img src="./assets/img/posts/20210402/post7-fly-vision.jpg" alt="The fly vision" /></p>

<p>The perceptron is just a math operation, one that multiplies certain input values with preset “parameters” (called weights) and adds up the resulting multiplications to generate a value.</p>

<p>Then the magic spark was ignited, the parameters (weights) of the perceptron could be “learnt” by a process of minimizing the difference between known results of particular observations, and what the perceptron is actually calculating. It is this process of learning what we call <strong>training the neural network</strong>.</p>

<tweet>This idea is so powerful that even today it is one of the fundamental building blocks of what we call AI.</tweet>

<p>From this I will try to explain how this simple concept can have such diverse applications as natural language processing (think Alexa), image recognition like medical diagnosis from a CTR scan, autonomous vehicles, etc.</p>

<p>A basic neural network is a combination of perceptrons in different arrangements, the perceptron therefore was downgraded from “fly brain” to “network neuron”.
<img src="./assets/img/posts/20210402/post7-multilayer-perceptron.png" alt="A multilayer perceptron" /></p>

<p>A neural network has different components, in its basic form it has:</p>
<ul>
  <li>Input</li>
  <li>Hidden layers</li>
  <li>Output</li>
</ul>

<p><img src="./assets/img/posts/20210228/nnet_flow.gif" alt="Neural network components" /></p>

<h3 id="input">Input</h3>

<p>The inputs of a neural network are in their essence just numbers, therefore anything that can be converted to a number can become an input. Letters in a text, pixels in an image, frequencies in a sound wave, values from a sensor, etc. are all different things that when converted to a numerical value serve as inputs for the neural network. This is one of the reasons why applications of neural networks are so diverse.</p>

<p>Inputs can be as many as one need for the task at hand, from maybe 9 inputs to teach a neural network how to play tic-tac-toe to thousands of pixels from a camera for an autonomous vehicle. Since the input of a perceptron needs to be a single value, if for example a color pixel is chosen as input, it most likely will be broken into three different values; its  red, green and blue components, hence each pixel will become 3 different inputs for the neural network.</p>

<h3 id="hidden-layers">Hidden layers</h3>

<p>A “layer” within a neural network is just a group of perceptrons that all perform the same exact mathematical operation to the inputs and produce an output. The catch is that each of them have different weights (parameters), therefore their output for a given input will be different amongst them. There are many types of layers, the most typical of them being a “dense” layer, which is another word to say that all the inputs are connected to all the neurons (individual perceptrons), and as said before, each of these connections have a weight associated with it, so that the operation that each neuron performs is a simple weighted sum of all the inputs.</p>

<p><img src="./assets/img/posts/20210402/post7-dense-layers.png" alt="post7-dense-layers" /></p>

<p>The hidden layer is then typically connected to another dense layer, and their connection means that each output of a neuron from the first layer is treated effectively as an input for the subsequent one, and it is thus connected to every neuron.</p>

<p>A neural network can have from one to as many layers as one can think, and the number of layers depends solely on the experience we have gathered on the particular problem we would like to solve.</p>

<p>Another critical parameter of a hidden layer is the number of neurons it has, and again, we need to rely on experience to determine how many neurons are needed for a given problem. I have seen networks that vary from a couple of neurons to the thousands. And of course each hidden layer can have as many neurons as we please, so the number of combinations is vast.</p>

<p>To the number of layers, their type and how many neurons each have, is what we call the <em>network topology</em> (including the number of inputs and outputs).</p>

<h3 id="output">Output</h3>

<p>At the very end of the chain, another layer lies (which behaves just like a hidden layer), but has the peculiarity that it is the final layer, and therefore whatever it calculates will be the output values of the whole network. The number of outputs the network has is a function of the problem we would like to solve. It could be as simple as one output, with its value representing a probability of an action (like in the case of the flee reaction of the housefly), to many outputs, perhaps if our network is trying to distinguish images of animals, one would have an output for each animal species, and the output would represent how much confidence the network has that the particular image belongs to the corresponding species.</p>

<p>As we said, the neural network is just a collection of individual neurons, doing basic math operations on certain inputs in series of layers that eventually generate an output. This mesh of neurons is then “trained” on certain output values from known cases of the inputs; once it has learned it can then process new inputs, values that it has never seen before with surprisingly accurate results.</p>

<p>Many of the problems neural networks solve, could be certainly worked out by other algorithms, however, since neural networks are in their core very basic operations, once trained, they are extremely efficient, hence much quicker and economical to produce results.</p>

<p>There are a few more details on how a simple neural network operate that I purposedly left out to make this explanation as simple as possible. Thinks like biases, the activation functions and the math behind learning, the backpropagation algorithm, I will leave to a more in depth article. I will also write (perhaps in a series) about the more complex topologies combining different types of layers and other building blocks, a part from the perceptron.</p>

<p><img src="./assets/img/posts/20210402/post7-alexa.png" alt="Alexa recognizing speach" /></p>

<p>Things like “Alexa”, are a bit more complex, but work on exactly the same principles. Let’s break down for example the case of asking “Alexa” to play a song in spotify. Alexa uses several different neural networks to acomplish this:</p>

<h4 id="1-speech-recognition">1. Speech recognition</h4>

<p>As a basic input we have our speech: the command <strong>“Alexa, play Van Halen”</strong>. This might seem quite simple for us humans to process, but for a machine is an incredible difficult feat to be able to understand speech, things like each individual voice timbre, entonation, intention and many more nuances of human spoken language make it so that traditional algorithms have struggled a lot with this. In our simplified example let’s say that we use a neural network to transform our spoken speech into text characters a computer is much more familiarized to learn.</p>

<h4 id="2-understanding-what-we-mean-natural-language-understanding">2. Understanding what we mean (Natural Language Understanding)</h4>

<p>Once the previous network managed to succesfuly convert our spoken words into text, there comes the even more difficult task of making sense of what we said. Things that we humans take for granted such as context, intonation and non verbal communication, help give our words meaning in a very subtle, but powerful way, a machine will have to do with much less information to correctly understand what we mean. It has to correctly identify the intention of our sentence and the subject or entities of what we mean.</p>

<p><img src="./assets/img/posts/20210402/post7-alexa-natural-lang.png" alt="post7-alexa-natural-lang" /></p>

<p>The neural network has to identify that it received a command (by identifying its name), the command (“play music”), and our choice (“Van Halen”). And it does so by means of simple math operations as described before. Of course the network involved is quite complex and has different types of neurons and connection types, but the underlying principles remain.</p>

<h4 id="3-replying-to-us">3. Replying to us</h4>

<p>Once Alexa understood what we meant, it then proceeds to execute the action of the command it interpreted and it replies to us in turn using natural language. This is accomplished using a technique called speech synthesis, things like pitch, duration and intensity of the words and phonems are selected based on the “meaning” of what Alexa will respond to us: “Playing songs by Van Halen on Spotify” sounding quite naturally. And all is accomplished with neural networks executing many simple math operations.</p>

<p><img src="./assets/img/posts/20210402/post7-alexa-steps.png" alt="post7-alexa-steps" />
<small>Although it seems quite complex, the process for AI to understand us can be boiled down to simple math operations</small></p>

<p>Of course Amazon’s Alexa neural networks have undergone quite a lot of training to get to the level where they are, the beauty is that once trained, to perform their magic they just need a few mathematical operations.</p>

<p>As said before, I will continue to write about the basics of neural networks, the next article in the series will dive a bit deeper into the math behind a basic neural network.</p>]]></content><author><name>Armando Maynez</name></author><category term="theory" /><category term="neural networks" /><category term="machine learning" /><category term="artificial intelligence" /><summary type="html"><![CDATA[ELI5: what is a neural network.]]></summary></entry><entry><title type="html">Starting the adventure</title><link href="http://localhost:4000/starting-the-adventure.html" rel="alternate" type="text/html" title="Starting the adventure" /><published>2021-03-24T00:00:00+09:00</published><updated>2021-03-24T00:00:00+09:00</updated><id>http://localhost:4000/starting-the-adventure</id><content type="html" xml:base="http://localhost:4000/starting-the-adventure.html"><![CDATA[<p>In the midst of a global pandemic caused by the SARS-COV2 coronavirus; I decided to start blogging. I wanted to blog since a long time, I have always enjoyed writing, but many unknowns and having “no time” for it prevented me from taking it up. Things like: “I don’t really know who my target audience is”, “what would my topic or topics be?”, “I don’t think I am a world-class expert in anything”, and many more kept stopping me from setting up my own blog. Now seemed like a good time as any so with those and tons of other questions in my mind I decided it was time to start.</p>

<p>Funnily, this is not my first post. The birth of the blog came very natural as a way to “document” my newly established pursuit for getting myself into Machine Learning. This new adventure of mine comprises several things, and if I want to succeed I need to be serious about them all:</p>
<ul>
  <li>I want to start coding again! I used to code a long time ago, starting when I was 8 years old in a Tandy Color Computer hooked up to my parent’s TV.</li>
  <li>Machine Learning is a vast, wide subject, I want to learn the generals, but also to select a few areas to focus on.</li>
  <li>Setting up a blog to document my journey and share it:</li>
  <li>Establish a learning and blogging routine. If I don’t do this, I am sure this endeavour will die off soon.</li>
</ul>

<p>As for the focus areas I will start with:</p>
<ul>
  <li>Neural Networks fundamentals: history, basic architecture and math behind them</li>
  <li>Deep Neural Networks</li>
  <li>Reinforcement Learning</li>
  <li>Current state of the art: what is at the cutting edge now in terms of Deep Neural Networks and Reinforcement Learning?</li>
</ul>

<p>I selected the above areas to focus on based on my personal interests, I have been fascinated by the developments in reinforcement learning for a long time, in particular <a href="https://deepmind.com/blog">Deep Mind’s</a> awesome <a href="https://deepmind.com/blog/article/innovations-alphago">Go</a>, <a href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go">Chess</a> and <a href="https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning">Starcraft</a> playing agents. Therefore, I started reading a lot about it and even started a personal project for coding a <a href="./deep-q-learning-tic-tac-toe.html">tic-tac-toe learning agent</a>.</p>

<p>With my limited knowledge I have drafted the following learning path:</p>

<ol>
  <li>Youtube: <a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw">Three Blue One Brown’s</a> videos on <a href="https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">Neural Networks</a>, <a href="https://youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">Calculus</a> and <a href="https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Linear Algebra</a>. I cannot recommend them enough, they are of sufficient depth and use animation superbly to facilitate the understanding of the subjects.</li>
  <li>Coursera: <a href="https://www.coursera.org/learn/machine-learning">Andrew Ng’s Machine Learning course</a></li>
  <li>Book: <a href="https://www.amazon.com/dp/1617294438/ref=cm_sw_em_r_mt_dp_AV4DHT7CVE95D1SR2JJ8">Deep Learning with Python by Francois Chollet</a></li>
  <li>Book: <a href="https://www.amazon.com/dp/0262193981/ref=cm_sw_em_r_mt_dp_10B6J4MDB7QE3YHBQF4X">Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto</a></li>
</ol>

<p>As for practical work I decided to start by <a href="./ML-Library-from-scratch.html">coding my first models from scratch</a> (without using libraries such as Tensorflow), to be able to deeply understand the math and logic behind the models, so far it has proven to be priceless.</p>

<p>For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.</p>

<p>I will continue to write about my learning road, what I find interesting and relevant, and to document all my practical exercises, as well as news and the state of the art in the world of AI.</p>

<p>So far, all I have learned has been so engaging that I am seriously thinking of a career change. I have 17 years of international experience in multinational corporations across various functions, such as Information Services, Sales, Customer Care and New Products Introduction, and sincerely, I am finding more joy in artificial intelligence than anything else I have worked on before. Let’s see where the winds take us.</p>

<p>Thanks for reading!</p>

<h3 id="ps-for-the-geeks-like-me-here-is-a-snippet-on-the-technical-side-of-the-blog">P.S. For the geeks like me, here is a snippet on the technical side of the blog.</h3>

<h4 id="static-website-generator">Static Website Generator</h4>
<p>I researched a lot on this, when I started I didn’t even know I needed a static website generator. I was just sure of one thing, I wanted my blog site to look modern, be easy to update and not to have anything extra or additional content or functionality I did not need.</p>

<p>There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are:</p>
<ul>
  <li><a href="https://wordpress.com/">wordpress</a></li>
  <li><a href="https://www.wix.com/">wix</a></li>
  <li><a href="https://www.squarespace.com/">squarespace</a></li>
  <li><a href="https://ghost.org/">ghost</a></li>
  <li><a href="https:webflow.com">webflow</a></li>
  <li><a href="https://www.netlify.com/">netlify</a></li>
  <li><a href="https://gohugo.io/">hugo</a></li>
  <li><a href="https://www.gatsbyjs.com/docs/glossary/static-site-generator/">gatsby</a></li>
  <li><a href="https://jekyllrb.com/">jekyll</a></li>
</ul>

<p>I started with the web interfaced generators with included hosting in their offerings:</p>

<p><a href="https://wordpress.com/">wordpress</a> is the old standard, it is the one CMS I knew from before, and I thought I needed a fully fledged CMS, so I blindly ran towards it. Turns out, it has grown a lot since I remembered, it is now a fully fledged platform for complex websites and ecommerce development, even so I decided to give it a try, I picked a template and <a href="https://amaynez.wordpress.com/">created a site</a>. Even with the most simplistic and basic template I could find, there is a lot going on in the site. Setting it up was not as difficult or cumbersome as others claim, it took me about one hour to have it up and running, it looks good, but a bit crowded for my personal taste, and I found out it serves ads in your site for the readers, that is a big no for me.</p>

<p>I have tried <a href="https://www.wix.com/">wix</a> and <a href="https://www.squarespace.com/">squarespace</a> before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.</p>

<p>I discovered <a href="https://ghost.org/">ghost</a> as the platform used by one of the bloggers I follow (<a href="https://ruder.io/">Sebastian Ruder</a>), turns out is a fantastic evolution over wordpress. It runs on the latest technologies, its interface is quite modern, and it is focused on one thing only: publishing. They have a paid hosting service, but the software is open sourced, therefore free to use in any hosting.</p>

<p>I also tested webflow and even created a <a href="https://armando-maynez.webflow.io">mockup</a> there, the learning curve was quite smooth, and its CMS seems quite robust, but a bit too much for the functionalities I required.</p>

<p>Next were the generators that don’t have a web interface, but can be easily set up:</p>

<p>The first I tried was <a href="https://www.netlify.com/">netlify</a>, I also set up a <a href="https://amaynez.netlify.app/">test site</a> in it. Netlify provides free hosting, and to keep your source files it uses GitHub (a repository keeps the source files where it publishes from). It has its own CMS, Netlify CMS, and you have a choice of site generators: Hugo, Gatsby, MiddleMan, Preact CLI, Next.js, Elevently and Nuxt.js, and once you choose there are some templates for each. I did not find the variety of templates enticing enough, and the set up process was much more cumbersome than with wordpress (at least for my knowledge level). I choose Hugo for my test site.</p>

<p>I also tested <a href="https://www.gatsbyjs.com/docs/glossary/static-site-generator/">gatsby</a> with it’s own Gatsby Cloud hosting service, <a href="https://amaynez.gatsbyjs.io/">here is my test site</a>. They also use GitHub as a base to host the source files to build the website, so you create a repository, and it is connected to it. I found the free template offerings quite limited for what I was looking for.</p>

<p>Finally it came the turn for <a href="https://jekyllrb.com/">jekyll</a>, although an older, and slower generator (compared to Hugo and Gatsby), it was created by one of the founders of GitHub, so it’s integration with GitHub Pages is quite natural and painless, so much so, that to use them together you don’t even have to install Jekyll in your machine! You have two choices:</p>
<ol>
  <li>keep it all online, by having one repository in Github keep all the source files, modify or add them online, and having Jekyll build and publish your site to the special <code class="language-plaintext highlighter-rouge">gh-pages</code> repository everytime you change or add a new file to the source repository.</li>
  <li>Have a synchronized local copy of the source files for the website, this way you can edit your blog and customize it in your choice of IDE (Integrated Development Environment). Then, when you update any file on your computer, you just “push” the changes to GitHub, and GitHub Pages automatically uses Jekyll to build and publish your site.</li>
</ol>

<p>I chose the second option, specially because I can manipulate files, like images, in my laptop, and everytime I sync my local repository with GitHub, they are updated and published automatically. Quite convenient.</p>

<p>After testing with several templates to get the feel for it, I decided to keep Jekyll for my blog for several reasons: the convenience of not having to install anything extra on my computer to build my blog, the integration with GitHub Pages, the ease of use, the future proofing via integration with modern technologies such as react or vue and the vast online community that has produced tons of templates and useful information for issue resolution, customization and added functionality.</p>

<p>I picked up a template, just forked the repository and started modifying the files to customize it, it was fast and easy, I even took it upon myself to add some functionality to the template (it served as a coding little project) like:</p>
<ul>
  <li>SEO meta tags</li>
  <li>Dark mode (<a href="https://github.com/the-mvm/the-mvm.github.io/blob/a8d4f781bfbc4107b4842433701d28f5bbf1c520/_config.yml#L10">configurable in _config.yml file</a>)</li>
  <li>automatic <a href="http://the-mvm.github.io/sitemap.xml">sitemap.xml</a></li>
  <li>automatic <a href="http://the-mvm.github.io/archive/">archive page</a> with infinite scrolling capability</li>
  <li><a href="https://the-mvm.github.io/tag/?tag=Coding">new page</a> of posts filtered by a single tag (without needing autopages from paginator V2), also with infinite scrolling</li>
  <li>click to tweet functionality (just add a <code class="language-plaintext highlighter-rouge">&lt;tweet&gt; &lt;/tweet&gt;</code> tag in your markdown.</li>
  <li>custom and responsive <a href="https://the-mvm.github.io/404.html">404 page</a></li>
  <li>responsive and automatic Table of Contents (optional per post)</li>
  <li>read time per post automatically calculated</li>
  <li>responsive post tags and social share icons (sticky or inline)</li>
  <li>included linkedin, reddit and bandcamp icons</li>
  <li><em>copy link to clipboard</em> sharing option (and icon)</li>
  <li>view on github link button (optional per post)</li>
  <li>MathJax support (optional per post)</li>
  <li>tag cloud in the home page</li>
  <li>‘back to top’ button</li>
  <li>comments ‘courtain’ to mask the disqus interface until the user clicks on it (<a href="https://github.com/the-mvm/the-mvm.github.io/blob/d4a67258912e411b639bf5acd470441c4c219544/_config.yml#L13">configurable in _config.yml</a>)</li>
  <li><a href="https://github.com/the-mvm/the-mvm.github.io/blob/d4a67258912e411b639bf5acd470441c4c219544/assets/css/main.css#L8">CSS variables</a> to make it easy to customize all colors and fonts</li>
  <li>added several pygments themes for code syntax highlight <a href="https://github.com/the-mvm/the-mvm.github.io/blob/e146070e9348c2e8f46cb90e3f0c6eb7b59c041a/_config.yml#L44">configurable from the _config.yml file</a>. See the <a href="https://github.com/the-mvm/the-mvm.github.io/tree/main/assets/css/highlighter">highlighter directory</a> for reference on the options.</li>
  <li>responsive footer menu and footer logo (<a href="https://github.com/the-mvm/the-mvm.github.io/blob/d4a67258912e411b639bf5acd470441c4c219544/_config.yml#L7">if setup in the config file</a>)</li>
  <li>smoother menu animations</li>
</ul>

<p><img src="./assets/img/template_screenshots/homepage-responsive.jpg" alt="my new blog" /></p>

<p><img src="./assets/img/template_screenshots/light-toggle.png" alt="night theme toggle" /></p>

<p>As a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites, but their complexity I think makes them useful for a big site with plenty of posts. For a small site like mine, Jekyll provides sufficient functionality and power without the hassle.</p>

<p>You can use the modified template yourself by <a href="https://github.com/the-mvm/the-mvm.github.io/fork/">forking my repository</a>. Let me know in the comments or feel free to contact me if you are interested in a detailed walkthrough on how to <a href="https://github.com/the-mvm/the-mvm.github.io#Installation">set it all up</a>.</p>

<h4 id="hosting">Hosting</h4>
<p>Since I decided on Jekyll to generate my site, the choice for hosting was quite obvious, <strong><a href="https://pages.github.com">Github Pages</a></strong> is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (<a href="https://the-mvm.github.io">the-mvm.github.io</a>).</p>

<h5 id="interplanetary-file-system">Interplanetary File System</h5>
<p>To contribute to and test <a href="https://github.com/ipfs/ipfs#quick-summary">IPFS</a> I also set up a <a href="https://weathered-bread-8229.on.fleek.co/">mirror</a> in IPFS by using <a href="https://fleek.co">fleek.co</a>. I must confess that it was more troublesome than I imagined, it was definetively not plug and play because of the paths used to fetch resources. The nature of IPFS makes short absolute paths for website resources (like images, css and javascript files) inoperative; the easiest fix for this is to use relative paths, however the same relative path that works for the root directory (i.e. <code class="language-plaintext highlighter-rouge">/index.html</code>) does not work for links inside directories (i.e. <code class="language-plaintext highlighter-rouge">/tags/</code>), and since the site is static, while generating it, one must make the distinction between the different directory levels for the page to be rendered correctly.</p>

<p>At first I tried a simple (but brute force solution):</p>

<pre><code class="language-jekyll"># determine the level of the current file
{% assign lvl = page.url | append:'X' | split:'/' | size %}
# create the relative base (i.e. "../")
{% capture relativebase %}{% for i in (3..lvl) %}../{% endfor %}{% endcapture %}
{% if relativebase == '' %}
	{% assign relativebase = './' %}
{% endif %}
...
# Eliminate unecesary double backslashes
{% capture post_url %}{{ relativebase }}{{ post.url }}{% endcapture %}
{% assign post_url = post_url | replace: "//", "/" %}
</code></pre>

<p>This <code class="language-plaintext highlighter-rouge">jekyll/liquid</code> code was executed in every page (or include) that needed to reference a resource hosted in the same server.</p>

<p>But this fix did not work for the search function, because it relies on a <code class="language-plaintext highlighter-rouge">search.json</code> file (also generated programmatically to be served as a static file), therefore when generating this file one either use the relative path for the <code class="language-plaintext highlighter-rouge">root</code> directory or for a nested directory, thus the search results will only link correctly the corresponding pages if the page where the user searched for something is in the corresponding scope.</p>

<p>So the final solution was to make the whole site flat, meaning to live in a single directory. All pages and posts will live under the root directory, and by doing so, I can control how to address the relative paths for resources.</p>]]></content><author><name>Armando Maynez</name></author><category term="general blogging" /><category term="thoughts" /><category term="life" /><summary type="html"><![CDATA[Midlife career change: a disaster or an opportunity?]]></summary></entry><entry><title type="html">Deep Q Learning for Tic Tac Toe</title><link href="http://localhost:4000/deep-q-learning-tic-tac-toe.html" rel="alternate" type="text/html" title="Deep Q Learning for Tic Tac Toe" /><published>2021-03-19T06:14:20+09:00</published><updated>2021-03-19T06:14:20+09:00</updated><id>http://localhost:4000/deep-q-learning-tic-tac-toe</id><content type="html" xml:base="http://localhost:4000/deep-q-learning-tic-tac-toe.html"><![CDATA[<center><img style="float: left;margin-right: 1em;" src="./assets/img/posts/20210318/Game_Screen.png" width="310" height="300" /></center>

<h2 id="background">Background</h2>
<p>After many years of a corporate career (17) diverging from computer science, I have now decided to learn Machine Learning and in the process return to coding (something I have always loved!).</p>

<p>To fully grasp the essence of ML I decided to start by <a href="./ML-Library-from-scratch.html">coding a ML library myself</a>, so I can fully understand the inner workings, linear algebra and calculus involved in Stochastic Gradient Descent. And on top learn Python (I used to code in C++ 20 years ago).</p>

<p>I built a general purpose basic ML library that creates a Neural Network (only DENSE layers), saves and loads the weights into a file, does forward propagation and training (optimization of weights and biases) using SGD. I tested the ML library with the XOR problem to make sure it worked fine. You can read the blog post for it <a href="./ML-Library-from-scratch.html">here</a>.</p>

<p>For the next challenge I am interested in reinforcement learning greatly inspired by Deep Mind’s astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe (or noughts and crosses).</p>

<p>How hard could it be?</p>

<p>Of course the first thing to do was to program the game itself, so I chose Python because I am learning it, so it gives me a good practice opportunity, and PyGame for the interface.
Coding the game was quite straightforward, albeit for the hiccups of being my first PyGame and almost my first Python program ever.
I created the game quite openly, in such a way that it can be played by two humans, by a human vs. an algorithmic AI, and a human vs. the neural network. And of course the neural network against a choice of 3 AI engines: random, <a href="https://en.wikipedia.org/wiki/Minimax">minimax</a> or hardcoded (an exercise I wanted to do since a long time).</p>

<p>While training, the visuals of the game can be disabled to make training much faster.
Now, for the fun part, training the network, I followed Deep Mind’s own DQN recommendations:</p>

<ul><li>The network will be an approximation for the Q value function or Bellman equation, meaning that the network will be trained to predict the "value" of each move available in a given game state.</li><li>A replay experience memory was implemented. This meant that the neural network will not be trained after each move. Each move will be recorded in a special "memory" alongside with the state of the board and the reward it received for taking such an action (move).</li><li>After the memory is sizable enough, batches of random experiences sampled from the replay memory are used for every training round</li><li>A secondary neural network (identical to the main one) is used to calculate part of the Q value function (Bellman equation), in particular the future Q values. And then it is updated with the main network's weights every <em>n</em> games. This is done so that we are not chasing a moving target.</li></ul>

<h2 id="designing-the-neural-network">Designing the neural network</h2>

<center><img src="./assets/img/posts/20210318/Neural_Network_Topology.png" width="540" /></center>
<p><br /></p>

<p>The Neural Network chosen takes 9 inputs (the current state of the game) and outputs 9 Q values for each of the 9 squares in the board of the game (possible actions). Obviously some squares are illegal moves, hence while training there was a negative reward given to illegal moves hoping that the model would learn not to play illegal moves in a given position.</p>

<p>I started out with two hidden layers of 36 neurons each, all fully connected and activated via ReLu. The output layer was initially activated using sigmoid to ensure that we get a nice value between 0 and 1 that represents the QValue of a given state action pair.</p>

<h2 id="the-many-models">The many models…</h2>
<h3 id="model-1---the-first-try">Model 1 - the first try</h3>

<p>At first the model was trained by playing vs. a “perfect” AI, meaning a <a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Game.py#L43">hard coded algorithm</a> that never looses and that will win if it is given the chance. After several thousand training rounds, I noticed that the Neural Network was not learning much; so I switched to training vs. a completely random player, so that it will also learn how to win. After training vs. the random player, the Neural Network seems to have made progress and is steadily diminishing the loss function over time.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_across_all_episodes.png" width="540" /></center>
<p><br /></p>

<p>However, the model was still generating many illegal moves, so I decided to modify the reinforcement learning algorithm to punish more the illegal moves. The change consisted in populating with zeros all the corresponding illegal moves for a given position at the target values to train the network. This seemed to work very well for diminishing the illegal moves:</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves.png" width="540" /></center>
<p><br /></p>

<p>Nevertheless, the model was still performing quite poorly winning only around 50% of games vs. a completely random player (I expected it to win above 90% of the time). This was after only training 100,000 games, so I decided to keep training and see the results:</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves2.png" width="540" />
<small>Wins: 65.46% Losses: 30.32% Ties: 4.23%</small></center>

<p>Note that when training restarts, the loss and illegal moves are still high in the beginning of the training round, and this is caused by the epsilon greedy strategy that prefers exploration (a completely random move) over exploitation, this preference diminishes over time.</p>

<p>After another round of 100,000 games, I can see that the loss function actually started to diminish, and the win rate ended up at 65%, so with little hope I decided to carry on and do another round of 100,000 games (about 2 hours in an i7 MacBook Pro):</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves3.png" width="540" />
<small>Wins: 46.40% Losses: 41.33% Ties: 12.27%</small></center>

<p>As you can see in the chart, the calculated loss not even plateaued, but it seemed to increase a bit over time, which tells me the model is not learning anymore. This was confirmed by the win rate decreasing with respect of the previous round to a meek 46.4% that looks no better than a random player.</p>

<h3 id="model-2---linear-activation-for-the-output">Model 2 - Linear activation for the output</h3>

<p>After not getting the results I wanted, I decided to change the output activation function to linear, since the output is supposed to be a Q value, and not a probability of an action.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves4.png" width="540" /><br />
<small>Wins: 47.60% Losses: 39% Ties: 13.4%</small></center>
<p><br /></p>

<p>Initially I tested with only 1000 games to see if the new activation function was working, the loss function appears to be decreasing, however it reached a plateau around a value of 1, hence still not learning as expected. I came across a <a href="https://github.com/bckenstler/CLR">technique by Brad Kenstler, Carl Thome and Jeremy Jordan</a> called Cyclical Learning Rate, which appears to solve some cases of stagnating loss functions in this type of networks. So I gave it a go using their Triangle 1 model.</p>

<p>With the cycling learning rate in place, still no luck after a quick 1,000 games training round; so I decided to implement on top a decaying learning rate as per the following formula:</p>

<center><img src="./assets/img/posts/20210318/lr_formula.jpeg" width="280" /></center>

<p>The resulting learning rate combining the cycles and decay per epoch is:</p>
<center><img src="./assets/img/posts/20210318/LR_cycle_decay.png" width="480" />
<small>Learning Rate = 0.1, Decay = 0.0001, Cycle = 2048 epochs,<br />
        max Learning Rate factor = 10x</small></center>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">true_epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">DECAY_RATE</span><span class="o">*</span><span class="n">true_epoch</span><span class="p">))</span>
<span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">CLR_ON</span><span class="p">:</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">true_epoch</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">max_lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span>
    <span class="n">cycle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">floor</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="p">(</span><span class="n">epoch</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">epoch</span><span class="o">/</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">cycle</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learning_rate</span><span class="o">+</span><span class="p">(</span><span class="n">max_lr</span><span class="o">-</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span><span class="p">.</span><span class="n">DECAY_RATE</span> <span class="o">=</span> <span class="n">learning</span> <span class="n">rate</span> <span class="n">decay</span> <span class="n">rate</span>
<span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span> <span class="o">=</span> <span class="n">multiplier</span> <span class="n">that</span> <span class="n">determines</span> <span class="n">the</span> <span class="nb">max</span> <span class="n">learning</span> <span class="n">rate</span>
<span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span> <span class="o">=</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">epochs</span> <span class="n">each</span> <span class="n">cycle</span> <span class="n">lasts</span>
</code></pre></div></div>
<p><br />With these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves6.png" width="540" />
<small>1,000,000 episodes, 7.5 million epochs with batches of 64 moves each<br />
Wins: 52.66% Losses: 36.02% Ties: 11.32%</small></center>

<p>After <strong>24 hours!</strong>, my computer was able to run 1,000,000 episodes (games played), which represented 7.5 million training epochs of batches of 64 plays (480 million plays learned), the learning rate did decreased (a bit), but is clearly still in a plateau; interestingly, the lower boundary of the loss function plot seems to continue to decrease as the upper bound and the moving average remains constant. This led me to believe that I might have hit a local minimum.
<a name="Model3"></a></p>
<h3 id="model-3---new-network-topology">Model 3 - new network topology</h3>

<p>After all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves7.png" width="540" />
<small>100,000 episodes, 635,000 epochs with batches of 64 moves each<br />
<b>Wins: 76.83%</b> Losses: 17.35% Ties: 5.82%</small></center>

<p>I increased to 200 neurons each hidden layer. In spite of this great improvement the loss function was still in a plateau at around 0.1 (Mean Squared Error). Which, although it is greatly reduced from what we had, still was giving out only 77% win rate vs. a random player, the network was playing tic tac toe as a toddler!</p>

<center><img src="./assets/img/posts/20210318/Game_Screen2.png" width="240" height="240" />
<small>*I can still beat the network most of the time! (I am playing with the red X)*</small></center>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves10.png" width="540" />
<small>100,000 more episodes, 620,000 epochs with batches of 64 moves each<br />
<b>Wins: 82.25%</b> Losses: 13.28% Ties: 4.46%</small></center>

<p><strong>Finally we crossed the 80% mark!</strong> This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.</p>

<p>After more training rounds and some experimenting with the learning rate and other parameters, I couldn’t improve past the 82.25% win rate.</p>

<p>These have been the results so far:</p>

<center><img src="./assets/img/posts/20210318/Models1to3.png" width="540" /></center>
<p><br /></p>

<p>It is quite interesting to learn how the many parameters (hyper-parameters as most authors call them) of a neural network model affect its training performance, I have played with:</p>
<ul>
  <li>the learning rate</li>
  <li>the network topology and activation functions</li>
  <li>the cycling and decaying learning rate parameters</li>
  <li>the batch size</li>
  <li>the target update cycle (when the target network is updated with the weights from the policy network)</li>
  <li>the rewards policy</li>
  <li>the epsilon greedy strategy</li>
  <li>whether to train vs. a random player or an “intelligent” AI.</li>
</ul>

<p>And so far the most effective change has been the network topology, but being so close but not quite there yet to my goal of 90% win rate vs. a random player, I will still try to optimize further.</p>

<tweet>Network topology seems to have the biggest impact on a neural network's learning ability.</tweet>

<p><a name="Model4"></a></p>
<h3 id="model-4---implementing-momentum">Model 4 - implementing momentum</h3>

<p>I <a href="https://www.reddit.com/r/MachineLearning/comments/lzvrwp/p_help_with_a_reinforcement_learning_project/">reached out to the reddit community</a> and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm. So I did some research and ended up deciding to implement various optimization methods to experiment with:</p>

<ul>
  <li>Stochastic Gradient Descent with Momentum</li>
  <li>RMSProp: Root Mean Square Plain Momentum</li>
  <li>NAG: Nezterov’s Accelerated Momentum</li>
  <li>Adam: Adaptive Moment Estimation</li>
  <li>and keep my old vanilla Gradient Descent (vGD) ☺</li>
</ul>

<p><a name="optimization"></a><a href="https://the-mvm.github.io/neural-network-optimization-methods/">Click here for a detailed explanation and code of all the implemented optimization algorithms.</a></p>

<p>So far, I have not been able to get better results with Model 4, I have tried all the momentum optimization algorithms with little to no success.
<a name="Model5"></a></p>
<h3 id="model-5---implementing-one-hot-encoding-and-changing-topology-again">Model 5 - implementing one-hot encoding and changing topology (again)</h3>
<p>I came across an <a href="https://github.com/AxiomaticUncertainty/Deep-Q-Learning-for-Tic-Tac-Toe/blob/master/tic_tac_toe.py">interesting project in Github</a> that deals exactly with Deep Q Learning, and I noticed that he used “one-hot” encoding for the input as opposed to directly entering the values of the player into the 9 input slots. So I decided to give it a try and at the same time change my topology to match his:</p>

<center><img src="./assets/img/posts/20210318/Neural_Network_Topology3.png" width="540" /></center>

<p>So, ‘one hot’ encoding is basically changing the input of a single square in the tic tac toe board to three numbers, so that each state is represented with different inputs, thus the network can clearly differentiate the three of them. As the original author puts it, the way I was encoding, having 0 for empty, 1 for X and 2 for O, the network couldn’t easily tell that, for instance, O and X both meant occupied states, because one is two times as far from 0 as the other. With the new encoding, the empty state will be 3 inputs: (1,0,0), the X will be (0,1,0) and the O (0,0,1) as in the diagram.</p>

<p>Still, no luck even with Model 5, so I am starting to think that there could be a bug in my code.</p>

<p>To test this hypothesis, I decided to implement the same model using Tensorflow / Keras.</p>

<p><a name="Model6"></a></p>
<h3 id="model-6---tensorflow--keras">Model 6 - Tensorflow / Keras</h3>
<center><img src="https://www.kubeflow.org/docs/images/logos/TensorFlow.png" width="100" height="100" /></center>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">hidden_layers</span><span class="p">:</span>
    <span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span>
                           <span class="n">units</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span>
                           <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                           <span class="n">input_dim</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                           <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                           <span class="n">bias_initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span>
                        <span class="n">outputs</span><span class="p">,</span>
                        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                        <span class="n">bias_initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
           <span class="n">beta_1</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">GAMMA_OPT</span><span class="p">,</span>
           <span class="n">beta_2</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">BETA</span><span class="p">,</span>
           <span class="n">epsilon</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">,</span>
           <span class="n">amsgrad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
                           <span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span>
                           <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>
<p>As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.</p>

<p>The training function changed to:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reduce_lr_on_plateau</span> <span class="o">=</span> <span class="nc">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">'loss'</span><span class="p">,</span>
                                         <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                         <span class="n">patience</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">states_to_train</span><span class="p">),</span>
                                 <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">targets_to_train</span><span class="p">),</span>
                                 <span class="n">epochs</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">EPOCHS</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                 <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">reduce_lr_on_plateau</span><span class="p">],</span>
                                 <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>With Tensorflow implemented, the first thing I noticed, was that I had an error in the calculation of the loss, although this only affected reporting and didn’t change a thing on the training of the network, so the results kept being the same, <strong>the loss function was still stagnating! My code was not the issue.</strong>
<a name="Model7"></a></p>
<h3 id="model-7---changing-the-training-schedule">Model 7 - changing the training schedule</h3>
<p>Next I tried to change the way the network was training as per <a href="https://www.reddit.com/user/elBarto015">u/elBarto015</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/lzzjar/i_created_an_ai_for_super_hexagon_based_on/gqc8ka6?utm_source=share&amp;utm_medium=web2x&amp;context=3">advised me on reddit</a>.</p>

<p>The way I was training initially was:</p>
<ul>
  <li>Games begin being simulated and the outcome recorded in the replay memory</li>
  <li>Once a sufficient ammount of experiences are recorded (at least equal to the batch size) the Network will train with a random sample of experiences from the replay memory. The ammount of experiences to sample is the batch size.</li>
  <li>The games continue to be played between the random player and the network.</li>
  <li>Every move from either player generates a new training round, again with a random sample from the replay memory.</li>
  <li>This continues until the number of games set up conclude.</li>
</ul>

<center><img src="./assets/img/posts/20210318/ReplayMemoryBefore.png" width="540" /></center>

<p>The first change was to train only after every game concludes with the same ammount of data (a batch). This was still not giving any good results.</p>

<p>The second change was more drastic, it introduced the concept of epochs for every training round, it basically sampled the replay memory for epochs * batch size experiences, for instance if epochs selected were 10, and batch size was 81, then 810 experiences were sampled out of the replay memory. With this sample the network was then trained for 10 epochs randomly using the batch size.</p>

<p>This meant that I was training now effectively 10 (or the number of epochs selected) times more per game, but in batches of the same size and randomly shuffling the experiences each epoch.</p>

<center><img src="./assets/img/posts/20210318/ReplayMemoryAfter.png" width="540" /></center>
<p><br /></p>

<p>After still playing around with some hyperparameters I managed to get similar performance as I got before, reaching 83.15% win rate vs. the random player, so I decided to keep training in rounds of 2,000 games each to evaluate performance. With almost every round I could see improvement:</p>

<center><img src="./assets/img/posts/20210318/Model7HyperParameters.png" width="540" /><br />
<img src="./assets/img/posts/20210318/Model7.png" width="480" />
</center>
<p><br /></p>

<p>As of today, my best result so far is 87.5%, I will leave it rest for a while and keep investigating to find a reason for not being able to reach at least 90%. I read about <a href="https://medium.com/applied-data-science/how-to-train-ai-agents-to-play-multiplayer-games-using-self-play-deep-reinforcement-learning-247d0b440717">self play</a>, and it looks like a viable option to test and a fun coding challenge. However, before embarking in yet another big change I want to ensure I have been thorough with the model and have tested every option correctly.</p>

<p>I feel the end is near… should I continue to update this post as new events unfold or shall I make it a multi post thread?</p>]]></content><author><name>Armando Maynez</name></author><category term="machine learning" /><category term="artificial intelligence" /><category term="reinforcement learning" /><category term="coding" /><category term="python" /><summary type="html"><![CDATA[Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?]]></summary></entry><entry><title type="html">Neural Network Optimization Methods and Algorithms</title><link href="http://localhost:4000/neural-network-optimization-methods.html" rel="alternate" type="text/html" title="Neural Network Optimization Methods and Algorithms" /><published>2021-03-13T04:32:20+09:00</published><updated>2021-03-13T04:32:20+09:00</updated><id>http://localhost:4000/neural-network-optimization-methods</id><content type="html" xml:base="http://localhost:4000/neural-network-optimization-methods.html"><![CDATA[<p>For the seemingly small project I undertook of <a href="./deep-q-learning-tic-tac-toe.html">creating a machine learning neural network that could learn by itself to play tic-tac-toe</a>, I bumped into the necesity of implementing at least one momentum algorithm for the optimization of the network during backpropagation.</p>

<p>And since my original post for the TicTacToe project is quite large already, I decided to post separately these optimization methods and how did I implement them in my code.</p>

<h3 id="adam">Adam</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#adam">source</a></p>

<p>Adaptive Moment Estimation (Adam) is an optimization method that computes adaptive learning rates for each weight and bias. In addition to storing an exponentially decaying average of past squared gradients \(v_t\) and an exponentially decaying average of past gradients \(m_t\), similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients \(m_t\) and \(v_t\) respectively as follows:</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
m_t &amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\<br />
v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2<br />
\end{split}<br />
\end{align}<br />
\)</p>
<p>\(m_t\) and \(v_t\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \(m_t\) and \(v_t\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \(\beta_1\) and \(\beta_2\) are close to 1).</p>
<p>They counteract these biases by computing bias-corrected first and second moment estimates:</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
\hat{m}_t &amp;= \dfrac{m_t}{1 - \beta^t_1} \\<br />
\hat{v}_t &amp;= \dfrac{v_t}{1 - \beta^t_2} \end{split}<br />
\end{align}<br />
\)</p>
<p>We then use these to update the weights and biases which yields the Adam update rule:</p>
<p style="text-align:center">\(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\).</p>
<p>The authors propose defaults of 0.9 for \(\beta_1\), 0.999 for \(\beta_2\), and \(10^{-8}\) for \(\epsilon\).</p>
<p><a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L243">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># decaying averages of past gradients
</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                        <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                        <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="p">))</span>

<span class="c1"># decaying averages of past squared gradients
</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span>
                        <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA2</span><span class="p">)</span>
                        <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                         <span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span>
                        <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA2</span><span class="p">)</span>
                        <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
                                         <span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                         <span class="p">))</span>

<span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">ADAM_BIAS_Correction</span><span class="p">:</span>
    <span class="c1"># bias-corrected first and second moment estimates
</span>    <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                          <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                          <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                          <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                          <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>

<span class="c1"># apply to weights and biases
</span><span class="n">weight_col</span> <span class="o">-=</span> <span class="p">((</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                      <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                      <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">))))</span>
<span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">((</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                        <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">))))</span>
</code></pre></div></div>

<h3 id="sgd-momentum">SGD Momentum</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#momentum">source</a></p>

<p>Vanilla SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.</p>
<p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction \(\gamma\) of the update vector of the past time step to the current update vector:</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
v_t &amp;= \beta_1 v_{t-1} + \eta \nabla_\theta J( \theta) \\<br />
\theta &amp;= \theta - v_t<br />
\end{split}<br />
\end{align}<br />
\)</p>
<p>The momentum term \(\beta_1\) is usually set to 0.9 or a similar value.</p>
<p>Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \(\beta_1 &lt; 1\)). The same thing happens to our weight and biases updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>
<p><a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L210">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                       <span class="o">+</span><span class="p">(</span><span class="n">eta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                       <span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                       <span class="o">+</span><span class="p">(</span><span class="n">eta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                       <span class="p">))</span>

<span class="n">weight_col</span> <span class="o">-=</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
<span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</code></pre></div></div>

<h3 id="nesterov-accelerated-gradient-nag">Nesterov accelerated gradient (NAG)</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient">source</a></p>

<p>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.</p>
<p>Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \(\beta_1 v_{t-1}\) to move the weights and biases \(\theta\). Computing \( \theta - \beta_1 v_{t-1} \) thus gives us an approximation of the next position of the weights and biases (the gradient is missing for the full update), a rough idea where our weights and biases are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current weights and biases \(\theta\) but w.r.t. the approximate future position of our weights and biases:</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
v_t &amp;= \beta_1 v_{t-1} + \eta \nabla_\theta J( \theta - \beta_1 v_{t-1} ) \\<br />
\theta &amp;= \theta - v_t<br />
\end{split}<br />
\end{align}<br />
\)</p>
<p>Again, we set the momentum term \(\beta_1\) to a value of around 0.9. While Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update. This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of Neural Networks on a number of tasks.</p>
<p>Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual weight and bias to perform larger or smaller updates depending on their importance.</p>
<p><a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L219">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v_prev</span> <span class="o">=</span> <span class="p">{</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span>
          <span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]}</span>

<span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
            <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">NAG_COEFF</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
           <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
<span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
            <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">NAG_COEFF</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
           <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="n">weight_col</span> <span class="o">+=</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">*</span> <span class="n">v_prev</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
               <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
<span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">*</span> <span class="n">v_prev</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
               <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
</code></pre></div></div>

<h3 id="rmsprop">RMSprop</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#rmsprop">source</a></p>

<p>RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Lecture 6e of his Coursera Class</a>.</p>
<p>RMSprop was developed stemming from the need to resolve other method's radically diminishing learning rates.</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
E[\theta^2]_t &amp;= \beta_1 E[\theta^2]_{t-1} + (1-\beta_1) \theta^2_t \\<br />
\theta_{t+1} &amp;= \theta_{t} - \dfrac{\eta}{\sqrt{E[\theta^2]_t + \epsilon}} \theta_{t}<br />
\end{split}<br />
\end{align}<br />
\)</p>
<p>RMSprop divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \(\beta_1\) to be set to 0.9, while a good default value for the learning rate \(\eta\) is 0.001.</p>
<p><a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L232">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                      <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                      <span class="o">+</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                      <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                        <span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                      <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                      <span class="o">+</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                      <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                        <span class="p">))</span>

<span class="n">weight_col</span> <span class="o">-=</span> <span class="p">(</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
              <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">)))</span>
              <span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
               <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">)))</span>
                <span class="p">)</span>
</code></pre></div></div>

<h3 id="complete-code">Complete code</h3>
<p>All in all the code ended up like this:
<a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L1">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">max_lr</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span>
    <span class="n">cycle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span>
                    <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">))</span>
                    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">epoch</span> <span class="o">/</span> <span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)</span>
        <span class="o">-</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">cycle</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learning_rate</span>
        <span class="o">+</span> <span class="p">(</span><span class="n">max_lr</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="p">)</span>
        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">apply_gradients</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">true_epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span>
            <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">DECAY_RATE</span> <span class="o">*</span> <span class="n">true_epoch</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">CLR_ON</span><span class="p">:</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">true_epoch</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">weight_col</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">'vanilla'</span><span class="p">:</span>
            <span class="n">weight_col</span> <span class="o">-=</span> <span class="n">eta</span>
                        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="o">/</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>
            <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">eta</span>
                        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="o">/</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>

        <span class="k">elif</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">'SGD_momentum'</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                                   <span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                                   <span class="o">+</span><span class="p">(</span><span class="n">eta</span>
                                   <span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                                   <span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                                   <span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                                   <span class="o">+</span><span class="p">(</span><span class="n">eta</span>
                                   <span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                                   <span class="p">))</span>

            <span class="n">weight_col</span> <span class="o">-=</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>

        <span class="k">elif</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">'NAG'</span><span class="p">:</span>
            <span class="n">v_prev</span> <span class="o">=</span> <span class="p">{</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span>
                      <span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]}</span>

            <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                        <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">NAG_COEFF</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                       <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                        <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">NAG_COEFF</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                       <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

            <span class="n">weight_col</span> <span class="o">+=</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">*</span> <span class="n">v_prev</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                           <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">*</span> <span class="n">v_prev</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                           <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>

        <span class="k">elif</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">'RMSProp'</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                            <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                            <span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                            <span class="o">+</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                            <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                            <span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                            <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                            <span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                            <span class="o">+</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                            <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                            <span class="p">))</span>

            <span class="n">weight_col</span> <span class="o">-=</span> <span class="p">(</span><span class="n">eta</span>
                          <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                          <span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">)))</span>
                          <span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">eta</span>
                          <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                          <span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">)))</span>
                            <span class="p">)</span>

        <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">"ADAM"</span><span class="p">:</span>
            <span class="c1"># decaying averages of past gradients
</span>            <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span>
                                <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                              <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                              <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                              <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                                    <span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span>
                                <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                              <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                              <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                              <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                                    <span class="p">))</span>

            <span class="c1"># decaying averages of past squared gradients
</span>            <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span>
                                    <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                                    <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA2</span><span class="p">)</span>
                                    <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span>
                                            <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
                                                <span class="n">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                                     <span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span>
                                    <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                                    <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA2</span><span class="p">)</span>
                                    <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span>
                                            <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
                                                <span class="n">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                                     <span class="p">))</span>

            <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">ADAM_BIAS_Correction</span><span class="p">:</span>
                <span class="c1"># bias-corrected first and second moment estimates
</span>                <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                                <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                              <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
                <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                                <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                              <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
                <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                                <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                              <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
                <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                                <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                              <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>

            <span class="c1"># apply to weights and biases
</span>            <span class="n">weight_col</span> <span class="o">-=</span> <span class="p">((</span><span class="n">eta</span>
                            <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                            <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                            <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">))))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">((</span><span class="n">eta</span>
                            <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                            <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                            <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">))))</span>

    <span class="n">self</span><span class="p">.</span><span class="nf">gradient_zeros</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name>Armando Maynez</name></author><category term="coding" /><category term="machine learning" /><category term="optimization" /><category term="deep Neural networks" /><summary type="html"><![CDATA[Some neural network optimization algorithms mostly to implement momentum when doing back propagation.]]></summary></entry></feed>